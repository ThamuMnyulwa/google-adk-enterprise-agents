{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Tool Integration & Function Calling\n",
    "\n",
    "## Production Agent Capabilities with Enterprise Patterns\n",
    "\n",
    "**Module Duration:** 15 minutes | **Focus:** Advanced tool patterns with error handling\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "Master production-grade tool integration for enterprise agent systems:\n",
    "\n",
    "- **Advanced Tool Patterns:** Complex integrations beyond basic functions\n",
    "- **Error Recovery:** Production retry logic and fallback strategies\n",
    "- **Tool Composition:** Chaining multiple tools for complex workflows\n",
    "- **Resource Management:** Rate limiting and connection handling\n",
    "- **Real Integrations:** File processing, APIs, databases\n",
    "\n",
    "**What You'll Build:**\n",
    "- Agent with multiple production-ready tools\n",
    "- Error handling and retry mechanisms\n",
    "- Tool orchestration for complex tasks\n",
    "- Enterprise integration patterns\n",
    "\n",
    "This covers advanced tool patterns used in real production agent systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Tool Integration - Production Patterns\n",
    "import asyncio\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "from functools import wraps\n",
    "import sqlite3\n",
    "\n",
    "# Configure logging for production monitoring\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"üõ†Ô∏è ADVANCED TOOL INTEGRATION\")\n",
    "print(\"=\" * 35)\n",
    "print(f\"Session: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"Focus: Production tool patterns with error handling\")\n",
    "print()\n",
    "\n",
    "@dataclass\n",
    "class ToolExecutionResult:\n",
    "    \"\"\"Production tool execution tracking\"\"\"\n",
    "    tool_name: str\n",
    "    success: bool\n",
    "    result: Any\n",
    "    execution_time: float\n",
    "    error_message: Optional[str] = None\n",
    "    retry_count: int = 0\n",
    "\n",
    "print(\"‚úÖ Production tool execution framework initialized\")\n",
    "print(\"   Error tracking, retry logic, performance monitoring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Production Tool Decorators\n",
    "\n",
    "Enterprise tools require error handling, retry logic, and monitoring. These decorators provide:\n",
    "\n",
    "**Error Recovery Patterns:**\n",
    "- **Retry Logic:** Exponential backoff for transient failures\n",
    "- **Circuit Breaker:** Fail-fast when services are down\n",
    "- **Fallback Strategies:** Alternative approaches when primary tools fail\n",
    "- **Resource Management:** Rate limiting and connection pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production Tool Decorators for Enterprise Reliability\n",
    "\n",
    "def retry_on_failure(max_retries=3, delay=1.0, backoff=2.0):\n",
    "    \"\"\"Enterprise retry decorator with exponential backoff\"\"\"\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            last_exception = None\n",
    "            current_delay = delay\n",
    "            \n",
    "            for attempt in range(max_retries + 1):\n",
    "                try:\n",
    "                    start_time = time.time()\n",
    "                    result = func(*args, **kwargs)\n",
    "                    execution_time = time.time() - start_time\n",
    "                    \n",
    "                    logger.info(f\"{func.__name__} succeeded on attempt {attempt + 1} ({execution_time:.2f}s)\")\n",
    "                    return ToolExecutionResult(\n",
    "                        tool_name=func.__name__,\n",
    "                        success=True,\n",
    "                        result=result,\n",
    "                        execution_time=execution_time,\n",
    "                        retry_count=attempt\n",
    "                    )\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    last_exception = e\n",
    "                    logger.warning(f\"{func.__name__} failed on attempt {attempt + 1}: {e}\")\n",
    "                    \n",
    "                    if attempt < max_retries:\n",
    "                        logger.info(f\"Retrying in {current_delay:.1f}s...\")\n",
    "                        time.sleep(current_delay)\n",
    "                        current_delay *= backoff\n",
    "            \n",
    "            # All retries failed\n",
    "            return ToolExecutionResult(\n",
    "                tool_name=func.__name__,\n",
    "                success=False,\n",
    "                result=None,\n",
    "                execution_time=0.0,\n",
    "                error_message=str(last_exception),\n",
    "                retry_count=max_retries\n",
    "            )\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "def rate_limit(calls_per_minute=30):\n",
    "    \"\"\"Rate limiting decorator for API tools\"\"\"\n",
    "    call_times = []\n",
    "    \n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            current_time = time.time()\n",
    "            \n",
    "            # Remove calls older than 1 minute\n",
    "            call_times[:] = [t for t in call_times if current_time - t < 60]\n",
    "            \n",
    "            # Check rate limit\n",
    "            if len(call_times) >= calls_per_minute:\n",
    "                sleep_time = 60 - (current_time - call_times[0])\n",
    "                logger.info(f\"Rate limit reached for {func.__name__}. Waiting {sleep_time:.1f}s\")\n",
    "                time.sleep(sleep_time)\n",
    "            \n",
    "            call_times.append(current_time)\n",
    "            return func(*args, **kwargs)\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "print(\"‚úÖ Production tool decorators ready:\")\n",
    "print(\"   Retry logic with exponential backoff\")\n",
    "print(\"   Rate limiting for API compliance\")\n",
    "print(\"   Error tracking and performance monitoring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Tool Implementations\n",
    "\n",
    "Production agents require sophisticated tools that handle real-world complexity:\n",
    "\n",
    "**File Processing Tools:**\n",
    "- **Multi-format Support:** CSV, Excel, JSON, PDF processing\n",
    "- **Error Handling:** Corrupted files, encoding issues, size limits\n",
    "- **Performance:** Streaming for large files, memory management\n",
    "\n",
    "**API Integration Tools:**\n",
    "- **Authentication:** API keys, OAuth, token refresh\n",
    "- **Error Recovery:** Network timeouts, rate limits, service errors\n",
    "- **Data Validation:** Schema validation, type conversion\n",
    "\n",
    "**Database Tools:**\n",
    "- **Connection Management:** Pooling, reconnection, transactions\n",
    "- **Query Safety:** SQL injection prevention, parameter binding\n",
    "- **Performance:** Query optimization, result streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Tool Implementations with Production Patterns\n",
    "\n",
    "@retry_on_failure(max_retries=2)\n",
    "@rate_limit(calls_per_minute=20)\n",
    "def process_data_file(file_path: str, operation: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Advanced file processing with multiple format support and error handling.\n",
    "    \n",
    "    Handles CSV, Excel, JSON files with proper error recovery and validation.\n",
    "    Production features: encoding detection, size limits, memory management.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    # Validate file exists and size\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    file_size = os.path.getsize(file_path)\n",
    "    if file_size > 50 * 1024 * 1024:  # 50MB limit\n",
    "        raise ValueError(f\"File too large: {file_size / 1024 / 1024:.1f}MB (limit: 50MB)\")\n",
    "    \n",
    "    file_ext = os.path.splitext(file_path)[1].lower()\n",
    "    \n",
    "    try:\n",
    "        if file_ext == '.csv':\n",
    "            # CSV processing with encoding detection\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, encoding='utf-8')\n",
    "            except UnicodeDecodeError:\n",
    "                df = pd.read_csv(file_path, encoding='latin-1')\n",
    "            \n",
    "            if operation == 'analyze':\n",
    "                return {\n",
    "                    'rows': len(df),\n",
    "                    'columns': list(df.columns),\n",
    "                    'data_types': df.dtypes.to_dict(),\n",
    "                    'missing_values': df.isnull().sum().to_dict(),\n",
    "                    'sample_data': df.head(3).to_dict('records')\n",
    "                }\n",
    "            elif operation == 'summarize':\n",
    "                numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "                return {\n",
    "                    'total_rows': len(df),\n",
    "                    'numeric_summary': df[numeric_cols].describe().to_dict() if len(numeric_cols) > 0 else {},\n",
    "                    'categorical_summary': {col: df[col].value_counts().head().to_dict() \n",
    "                                          for col in df.select_dtypes(include=['object']).columns}\n",
    "                }\n",
    "                \n",
    "        elif file_ext in ['.xlsx', '.xls']:\n",
    "            # Excel processing with sheet handling\n",
    "            excel_file = pd.ExcelFile(file_path)\n",
    "            sheets_data = {}\n",
    "            \n",
    "            for sheet_name in excel_file.sheet_names[:3]:  # Limit to first 3 sheets\n",
    "                df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "                sheets_data[sheet_name] = {\n",
    "                    'rows': len(df),\n",
    "                    'columns': list(df.columns),\n",
    "                    'sample': df.head(2).to_dict('records')\n",
    "                }\n",
    "            \n",
    "            return {\n",
    "                'file_type': 'excel',\n",
    "                'sheets': sheets_data,\n",
    "                'total_sheets': len(excel_file.sheet_names)\n",
    "            }\n",
    "            \n",
    "        elif file_ext == '.json':\n",
    "            # JSON processing with validation\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            return {\n",
    "                'file_type': 'json',\n",
    "                'structure': type(data).__name__,\n",
    "                'size': len(data) if isinstance(data, (list, dict)) else 1,\n",
    "                'keys': list(data.keys()) if isinstance(data, dict) else None,\n",
    "                'sample': data[:2] if isinstance(data, list) else data\n",
    "            }\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type: {file_ext}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"File processing error: {e}\")\n",
    "        raise\n",
    "\n",
    "@retry_on_failure(max_retries=3, delay=2.0)\n",
    "@rate_limit(calls_per_minute=15)\n",
    "def fetch_api_data(endpoint: str, params: Dict[str, Any] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Production API integration with authentication and error handling.\n",
    "    \n",
    "    Features: timeout handling, response validation, structured error reporting.\n",
    "    Supports JSON APIs with proper HTTP status code handling.\n",
    "    \"\"\"\n",
    "    if params is None:\n",
    "        params = {}\n",
    "    \n",
    "    # Simulate different API endpoints for demonstration\n",
    "    if endpoint == 'weather':\n",
    "        # Mock weather API response\n",
    "        location = params.get('location', 'Unknown')\n",
    "        return {\n",
    "            'location': location,\n",
    "            'temperature': 22,\n",
    "            'humidity': 65,\n",
    "            'conditions': 'Partly Cloudy',\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'source': 'mock_weather_api'\n",
    "        }\n",
    "    \n",
    "    elif endpoint == 'stock_data':\n",
    "        # Mock stock data API\n",
    "        symbol = params.get('symbol', 'UNKNOWN')\n",
    "        return {\n",
    "            'symbol': symbol,\n",
    "            'price': 145.67,\n",
    "            'change': '+2.34',\n",
    "            'change_percent': '+1.63%',\n",
    "            'volume': 1234567,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'source': 'mock_stock_api'\n",
    "        }\n",
    "    \n",
    "    elif endpoint == 'news':\n",
    "        # Mock news API\n",
    "        topic = params.get('topic', 'general')\n",
    "        return {\n",
    "            'articles': [\n",
    "                {\n",
    "                    'title': f'Latest {topic} news update',\n",
    "                    'summary': f'Breaking news about {topic} developments.',\n",
    "                    'published': datetime.now().isoformat(),\n",
    "                    'source': 'Mock News API'\n",
    "                },\n",
    "                {\n",
    "                    'title': f'{topic.title()} market analysis',\n",
    "                    'summary': f'Expert analysis on {topic} trends.',\n",
    "                    'published': (datetime.now() - timedelta(hours=2)).isoformat(),\n",
    "                    'source': 'Mock News API'\n",
    "                }\n",
    "            ],\n",
    "            'total_results': 2,\n",
    "            'query': topic\n",
    "        }\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown API endpoint: {endpoint}\")\n",
    "\n",
    "@retry_on_failure(max_retries=2)\n",
    "def database_query(query_type: str, table: str, filters: Dict[str, Any] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Production database tool with connection management and query safety.\n",
    "    \n",
    "    Features: SQL injection prevention, connection pooling, transaction handling.\n",
    "    Supports basic CRUD operations with proper error handling.\n",
    "    \"\"\"\n",
    "    if filters is None:\n",
    "        filters = {}\n",
    "    \n",
    "    # Create in-memory database for demonstration\n",
    "    conn = sqlite3.connect(':memory:')\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    try:\n",
    "        # Create sample tables for demonstration\n",
    "        if table == 'users':\n",
    "            cursor.execute('''\n",
    "                CREATE TABLE users (\n",
    "                    id INTEGER PRIMARY KEY,\n",
    "                    name TEXT,\n",
    "                    email TEXT,\n",
    "                    created_date TEXT\n",
    "                )\n",
    "            ''')\n",
    "            \n",
    "            # Insert sample data\n",
    "            sample_users = [\n",
    "                (1, 'Alice Johnson', 'alice@example.com', '2024-01-15'),\n",
    "                (2, 'Bob Smith', 'bob@example.com', '2024-02-20'),\n",
    "                (3, 'Carol Davis', 'carol@example.com', '2024-03-10')\n",
    "            ]\n",
    "            cursor.executemany('INSERT INTO users VALUES (?, ?, ?, ?)', sample_users)\n",
    "            \n",
    "        elif table == 'orders':\n",
    "            cursor.execute('''\n",
    "                CREATE TABLE orders (\n",
    "                    id INTEGER PRIMARY KEY,\n",
    "                    user_id INTEGER,\n",
    "                    product TEXT,\n",
    "                    amount REAL,\n",
    "                    order_date TEXT\n",
    "                )\n",
    "            ''')\n",
    "            \n",
    "            sample_orders = [\n",
    "                (1, 1, 'Laptop', 999.99, '2024-01-20'),\n",
    "                (2, 2, 'Mouse', 29.99, '2024-02-25'),\n",
    "                (3, 1, 'Keyboard', 79.99, '2024-03-15')\n",
    "            ]\n",
    "            cursor.executemany('INSERT INTO orders VALUES (?, ?, ?, ?, ?)', sample_orders)\n",
    "        \n",
    "        # Execute query based on type\n",
    "        if query_type == 'select':\n",
    "            if filters:\n",
    "                # Build WHERE clause safely\n",
    "                where_parts = []\n",
    "                values = []\n",
    "                for key, value in filters.items():\n",
    "                    where_parts.append(f\"{key} = ?\")\n",
    "                    values.append(value)\n",
    "                \n",
    "                where_clause = \" AND \".join(where_parts)\n",
    "                query = f\"SELECT * FROM {table} WHERE {where_clause}\"\n",
    "                cursor.execute(query, values)\n",
    "            else:\n",
    "                cursor.execute(f\"SELECT * FROM {table}\")\n",
    "            \n",
    "            columns = [description[0] for description in cursor.description]\n",
    "            rows = cursor.fetchall()\n",
    "            \n",
    "            return {\n",
    "                'query_type': 'select',\n",
    "                'table': table,\n",
    "                'columns': columns,\n",
    "                'rows': [dict(zip(columns, row)) for row in rows],\n",
    "                'count': len(rows)\n",
    "            }\n",
    "            \n",
    "        elif query_type == 'count':\n",
    "            cursor.execute(f\"SELECT COUNT(*) FROM {table}\")\n",
    "            count = cursor.fetchone()[0]\n",
    "            \n",
    "            return {\n",
    "                'query_type': 'count',\n",
    "                'table': table,\n",
    "                'total_records': count\n",
    "            }\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported query type: {query_type}\")\n",
    "            \n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "print(\"\\nüîß Advanced Tools Ready:\")\n",
    "print(\"   File processing: CSV, Excel, JSON with error handling\")\n",
    "print(\"   API integration: Weather, stock, news with retry logic\")\n",
    "print(\"   Database tools: Safe queries with connection management\")\n",
    "print(\"   Production patterns: Rate limiting, monitoring, validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Integration with Advanced Tools\n",
    "\n",
    "Now we integrate these production tools with ADK agents. The agent will:\n",
    "\n",
    "**Tool Selection Logic:**\n",
    "- **Capability Matching:** Choose appropriate tools based on request type\n",
    "- **Error Recovery:** Fallback strategies when tools fail\n",
    "- **Resource Awareness:** Respect rate limits and connection pools\n",
    "- **Result Validation:** Verify tool outputs before responding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Advanced Tool Agent Ready:\n",
      " - File analysis: CSV, Excel, JSON\n",
      " - API data: Weather and stock prices\n",
      " - Database queries with filters\n",
      " - Multi-tool orchestration and reasoning\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Agent Integration with Advanced Tools (Google ADK 1.3.0)\n",
    "\n",
    "from google.adk.agents import Agent\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.models.lite_llm import LiteLlm\n",
    "from google.genai import types\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Track tool usage across runs (optional)\n",
    "tool_usage_counter = defaultdict(int)\n",
    "\n",
    "# ‚úÖ Tool definitions\n",
    "\n",
    "def analyze_file(file_path: str) -> str:\n",
    "    tool_usage_counter[\"analyze_file\"] += 1\n",
    "    print(f\"üîß Tool Called: analyze_file({file_path})\")\n",
    "\n",
    "    result = process_data_file(file_path, \"analyze\")\n",
    "\n",
    "    if hasattr(result, 'success') and result.success:\n",
    "        data = result.result\n",
    "        return f\"File Analysis: {data['rows']} rows, {len(data['columns'])} columns. Columns: {', '.join(data['columns'][:5])}{'...' if len(data['columns']) > 5 else ''}\"\n",
    "    return f\"File analysis failed: {getattr(result, 'error_message', 'Unknown error')}\"\n",
    "\n",
    "def get_weather_data(location: str) -> str:\n",
    "    tool_usage_counter[\"get_weather_data\"] += 1\n",
    "    print(f\"üîß Tool Called: get_weather_data({location})\")\n",
    "\n",
    "    result = fetch_api_data(\"weather\", {\"location\": location})\n",
    "\n",
    "    if hasattr(result, 'success') and result.success:\n",
    "        data = result.result\n",
    "    else:\n",
    "        data = result  # fallback\n",
    "    return f\"Weather in {data['location']}: {data['temperature']}¬∞C, {data['conditions']}, Humidity: {data['humidity']}%\"\n",
    "\n",
    "def get_stock_info(symbol: str) -> str:\n",
    "    tool_usage_counter[\"get_stock_info\"] += 1\n",
    "    print(f\"üîß Tool Called: get_stock_info({symbol})\")\n",
    "\n",
    "    result = fetch_api_data(\"stock_data\", {\"symbol\": symbol})\n",
    "\n",
    "    if hasattr(result, 'success') and result.success:\n",
    "        data = result.result\n",
    "    else:\n",
    "        data = result\n",
    "    return f\"Stock {data['symbol']}: ${data['price']} ({data['change']}, {data['change_percent']}), Volume: {data['volume']:,}\"\n",
    "\n",
    "def search_database(table: str, user_filter: str) -> str:\n",
    "    tool_usage_counter[\"search_database\"] += 1\n",
    "    print(f\"üîß Tool Called: search_database(table='{table}', user_filter='{user_filter}')\")\n",
    "\n",
    "    if not isinstance(user_filter, str):\n",
    "        print(f\"[‚ö†Ô∏è Warning] Unexpected type for user_filter: {type(user_filter)}. Forcing string.\")\n",
    "        user_filter = str(user_filter)\n",
    "\n",
    "    if user_filter.strip():\n",
    "        return f\"Filtered results from {table} where {user_filter}\"\n",
    "    return f\"All records from {table}\"\n",
    "\n",
    "# ‚úÖ Async setup function\n",
    "\n",
    "async def setup_advanced_agent():\n",
    "    \"\"\"Initialize agent with advanced tool capabilities\"\"\"\n",
    "\n",
    "    model = LiteLlm(model=\"ollama_chat/llama3.2:latest\")\n",
    "\n",
    "    # Optional Gemini switch:\n",
    "    # model = LiteLlm(\n",
    "    #     model=\"gemini/gemini-1.5-flash\",\n",
    "    #     api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    "    # )\n",
    "\n",
    "    agent = Agent(\n",
    "        name=\"AdvancedToolAgent\",\n",
    "        model=model,\n",
    "        instruction=\"\"\"\n",
    "You are a production-ready AI agent with advanced tool capabilities.\n",
    "\n",
    "You can:\n",
    "- Analyze structured data files (CSV, Excel, JSON)\n",
    "- Fetch real-time information like weather and stock prices\n",
    "- Search internal databases with or without filters\n",
    "- Use multiple tools together for complex tasks\n",
    "\n",
    "When using the 'search_database' tool, always pass 'user_filter' as plain text like 'age > 30' or an empty string. Do not pass JSON or objects.\n",
    "\n",
    "When using tools:\n",
    "1. Select the most appropriate tool\n",
    "2. Handle errors gracefully\n",
    "3. Always explain which tools you used and why\n",
    "4. Return clear and helpful responses\n",
    "\"\"\",\n",
    "        tools=[\n",
    "            analyze_file,\n",
    "            get_weather_data,\n",
    "            get_stock_info,\n",
    "            search_database\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    session_service = InMemorySessionService()\n",
    "    runner = Runner(agent=agent, app_name=\"advanced_tools\", session_service=session_service)\n",
    "\n",
    "    await session_service.create_session(\n",
    "        app_name=\"advanced_tools\",\n",
    "        user_id=\"system\",\n",
    "        session_id=\"main\"\n",
    "    )\n",
    "\n",
    "    return agent, runner, session_service\n",
    "\n",
    "# ‚úÖ Initialize the agent\n",
    "agent, runner, session_service = await setup_advanced_agent()\n",
    "\n",
    "print(\"\\n‚úÖ Advanced Tool Agent Ready:\")\n",
    "print(\" - File analysis: CSV, Excel, JSON\")\n",
    "print(\" - API data: Weather and stock prices\")\n",
    "print(\" - Database queries with filters\")\n",
    "print(\" - Multi-tool orchestration and reasoning\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Production Tool Demonstration\n",
    "\n",
    "Testing the agent with complex requests that require multiple tools and error handling:\n",
    "\n",
    "**Test Scenarios:**\n",
    "- **Multi-tool Requests:** Tasks requiring multiple integrated tools\n",
    "- **Error Recovery:** Handling tool failures gracefully\n",
    "- **Tool Chaining:** Using output from one tool as input to another\n",
    "- **Resource Management:** Respecting rate limits and timeouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:33:44 - LiteLLM:INFO\u001b[0m: utils.py:3101 - \n",
      "LiteLLM completion() model= llama3.2:latest; provider = ollama_chat\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= llama3.2:latest; provider = ollama_chat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ ADVANCED TOOL INTEGRATION TESTING\n",
      "==========================================\n",
      "\n",
      "üìã Test 1: Get the current weather in San Francisco and stock price for AAPL\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m09:33:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: ollama_chat/llama3.2:latest\n",
      "INFO:LiteLLM:selected model name for cost calculation: ollama_chat/llama3.2:latest\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:fetch_api_data succeeded on attempt 1 (0.00s)\n",
      "INFO:__main__:fetch_api_data succeeded on attempt 1 (0.00s)\n",
      "\u001b[92m09:33:59 - LiteLLM:INFO\u001b[0m: utils.py:3101 - \n",
      "LiteLLM completion() model= llama3.2:latest; provider = ollama_chat\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= llama3.2:latest; provider = ollama_chat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Tool Called: get_weather_data(San Francisco)\n",
      "üîß Tool Called: get_stock_info(AAPL)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m09:34:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: ollama_chat/llama3.2:latest\n",
      "INFO:LiteLLM:selected model name for cost calculation: ollama_chat/llama3.2:latest\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m09:34:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: ollama_chat/llama3.2:latest\n",
      "INFO:LiteLLM:selected model name for cost calculation: ollama_chat/llama3.2:latest\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Agent Response:\n",
      "The current weather in San Francisco is 22¬∞C with a partly cloudy sky and a humidity level of 65%. Additionally, the current stock price for Apple (AAPL) is $145.67, representing a 2.34% increase, or a +2.63% gain over the past year, with a volume of 1,234,567 shares traded today.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m09:34:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: ollama_chat/llama3.2:latest\n",
      "INFO:LiteLLM:selected model name for cost calculation: ollama_chat/llama3.2:latest\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m09:34:18 - LiteLLM:INFO\u001b[0m: utils.py:3101 - \n",
      "LiteLLM completion() model= llama3.2:latest; provider = ollama_chat\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= llama3.2:latest; provider = ollama_chat\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Test 2: Search the orders table where amount > 500\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m09:34:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: ollama_chat/llama3.2:latest\n",
      "INFO:LiteLLM:selected model name for cost calculation: ollama_chat/llama3.2:latest\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m09:34:30 - LiteLLM:INFO\u001b[0m: utils.py:3101 - \n",
      "LiteLLM completion() model= llama3.2:latest; provider = ollama_chat\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= llama3.2:latest; provider = ollama_chat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Tool Called: search_database(table='orders', user_filter='amount > 500')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m09:34:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: ollama_chat/llama3.2:latest\n",
      "INFO:LiteLLM:selected model name for cost calculation: ollama_chat/llama3.2:latest\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m09:34:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: ollama_chat/llama3.2:latest\n",
      "INFO:LiteLLM:selected model name for cost calculation: ollama_chat/llama3.2:latest\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m09:34:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: ollama_chat/llama3.2:latest\n",
      "INFO:LiteLLM:selected model name for cost calculation: ollama_chat/llama3.2:latest\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Agent Response:\n",
      "I used the 'search_database' tool to search for records in the 'orders' table, filtering only those with an 'amount' greater than 500. The result shows all records that meet this condition.\n",
      "\n",
      "Please note that I handled the filter parameter as plain text by escaping the '>' symbol with '\\u003e', since it has a special meaning in JSON.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:34:47 - LiteLLM:INFO\u001b[0m: utils.py:3101 - \n",
      "LiteLLM completion() model= llama3.2:latest; provider = ollama_chat\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= llama3.2:latest; provider = ollama_chat\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Test 3: Get weather for New York, then get stock info for TSLA and GOOGL\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m09:35:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: ollama_chat/llama3.2:latest\n",
      "INFO:LiteLLM:selected model name for cost calculation: ollama_chat/llama3.2:latest\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:fetch_api_data succeeded on attempt 1 (0.00s)\n",
      "INFO:__main__:fetch_api_data succeeded on attempt 1 (0.00s)\n",
      "INFO:__main__:fetch_api_data succeeded on attempt 1 (0.00s)\n",
      "\u001b[92m09:35:03 - LiteLLM:INFO\u001b[0m: utils.py:3101 - \n",
      "LiteLLM completion() model= llama3.2:latest; provider = ollama_chat\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= llama3.2:latest; provider = ollama_chat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Tool Called: get_weather_data(New York)\n",
      "üîß Tool Called: get_stock_info(TSLA)\n",
      "üîß Tool Called: get_stock_info(GOOGL)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m09:35:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: ollama_chat/llama3.2:latest\n",
      "INFO:LiteLLM:selected model name for cost calculation: ollama_chat/llama3.2:latest\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m09:35:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: ollama_chat/llama3.2:latest\n",
      "INFO:LiteLLM:selected model name for cost calculation: ollama_chat/llama3.2:latest\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Agent Response:\n",
      "The current weather in New York is 22¬∞C with a partly cloudy sky and a humidity level of 65%. Additionally, the current stock prices are:\n",
      "\n",
      "* Tesla (TSLA) at $145.67, representing a 2.34% increase.\n",
      "* Google (GOOGL) also at $145.67, showing a similar 2.34% increase. Both stocks have the same volume of 1,234,567 shares traded today.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m09:35:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: ollama_chat/llama3.2:latest\n",
      "INFO:LiteLLM:selected model name for cost calculation: ollama_chat/llama3.2:latest\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m09:35:26 - LiteLLM:INFO\u001b[0m: utils.py:3101 - \n",
      "LiteLLM completion() model= llama3.2:latest; provider = ollama_chat\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= llama3.2:latest; provider = ollama_chat\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Test 4: Find all orders in the database and get current weather in Chicago\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m09:35:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: ollama_chat/llama3.2:latest\n",
      "INFO:LiteLLM:selected model name for cost calculation: ollama_chat/llama3.2:latest\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:fetch_api_data succeeded on attempt 1 (0.00s)\n",
      "\u001b[92m09:35:40 - LiteLLM:INFO\u001b[0m: utils.py:3101 - \n",
      "LiteLLM completion() model= llama3.2:latest; provider = ollama_chat\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= llama3.2:latest; provider = ollama_chat\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m09:35:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: ollama_chat/llama3.2:latest\n",
      "INFO:LiteLLM:selected model name for cost calculation: ollama_chat/llama3.2:latest\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Tool Called: search_database(table='orders', user_filter='')\n",
      "üîß Tool Called: get_weather_data(Chicago)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m09:35:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: ollama_chat/llama3.2:latest\n",
      "INFO:LiteLLM:selected model name for cost calculation: ollama_chat/llama3.2:latest\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m09:35:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: ollama_chat/llama3.2:latest\n",
      "INFO:LiteLLM:selected model name for cost calculation: ollama_chat/llama3.2:latest\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Agent Response:\n",
      "I used the 'search_database' tool to search for all records in the 'orders' table. This resulted in a full return of all data from the 'orders' table.\n",
      "\n",
      "Then, I used the 'get_weather_data' tool to retrieve the current weather in Chicago. The result shows that the current weather in Chicago is 22¬∞C with a partly cloudy sky and a humidity level of 65%.\n",
      "\n",
      "üìä TOOL INTEGRATION ANALYSIS\n",
      "===================================\n",
      "üõ†Ô∏è Tool Invocation Notes:\n",
      "   - Tools are triggered implicitly based on model output.\n",
      "   - No need to define structured `function_call` schemas.\n",
      "   - Use print logs inside each tool to confirm execution.\n",
      "   - Use counters to track frequency and validate behavior.\n",
      "\n",
      "üî¢ TOOL USAGE SUMMARY\n",
      "   - get_weather_data: 3 call(s)\n",
      "   - get_stock_info: 3 call(s)\n",
      "   - search_database: 2 call(s)\n",
      "\n",
      "‚úÖ ADVANCED TOOL INTEGRATION COMPLETE\n",
      "   - Agent handled tool-based tasks using the configured LLM.\n",
      "   - Continue testing edge prompts to validate robustness.\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Production Tool Demonstration with Google ADK Agent (Any LLM via LiteLLM)\n",
    "\n",
    "from collections import defaultdict\n",
    "import asyncio\n",
    "\n",
    "# Counter to track tool usage across test runs\n",
    "tool_usage_counter = defaultdict(int)\n",
    "\n",
    "# Example for logging tool invocations:\n",
    "# def get_weather_data(city):\n",
    "#     tool_usage_counter[\"get_weather_data\"] += 1\n",
    "#     print(f\"[TOOL USED] get_weather_data({city})\")\n",
    "#     ...\n",
    "\n",
    "async def test_advanced_tools():\n",
    "    \"\"\"Run a suite of tests on an ADK-based agent with tool integration.\"\"\"\n",
    "\n",
    "    test_requests = [\n",
    "        \"Get the current weather in San Francisco and stock price for AAPL\",\n",
    "        \"Search the orders table where amount > 500\",\n",
    "        \"Get weather for New York, then get stock info for TSLA and GOOGL\",\n",
    "        \"Find all orders in the database and get current weather in Chicago\"\n",
    "    ]\n",
    "\n",
    "    print(\"\\nüß™ ADVANCED TOOL INTEGRATION TESTING\")\n",
    "    print(\"=\" * 42)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i, request in enumerate(test_requests, 1):\n",
    "        print(f\"\\nüìã Test {i}: {request}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        message = types.Content(\n",
    "            role=\"user\",\n",
    "            parts=[types.Part(text=request)]\n",
    "        )\n",
    "\n",
    "        response_text = \"\"\n",
    "\n",
    "        async for event in runner.run_async(\n",
    "            user_id=\"system\",\n",
    "            session_id=\"main\",\n",
    "            new_message=message\n",
    "        ):\n",
    "            if event.is_final_response():\n",
    "                response_text = event.content.parts[0].text\n",
    "                break\n",
    "\n",
    "        print(f\"ü§ñ Agent Response:\\n{response_text}\")\n",
    "        results.append({'request': request, 'response': response_text})\n",
    "\n",
    "        await asyncio.sleep(1)  # Avoid overwhelming APIs\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Run the tool demonstration\n",
    "demo_results = await test_advanced_tools()\n",
    "\n",
    "print(\"\\nüìä TOOL INTEGRATION ANALYSIS\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "print(\"üõ†Ô∏è Tool Invocation Notes:\")\n",
    "print(\"   - Tools are triggered implicitly based on model output.\")\n",
    "print(\"   - No need to define structured `function_call` schemas.\")\n",
    "print(\"   - Use print logs inside each tool to confirm execution.\")\n",
    "print(\"   - Use counters to track frequency and validate behavior.\")\n",
    "\n",
    "# Summary of tool usage\n",
    "if tool_usage_counter:\n",
    "    print(\"\\nüî¢ TOOL USAGE SUMMARY\")\n",
    "    for tool, count in tool_usage_counter.items():\n",
    "        print(f\"   - {tool}: {count} call(s)\")\n",
    "\n",
    "print(\"\\n‚úÖ ADVANCED TOOL INTEGRATION COMPLETE\")\n",
    "print(\"   - Agent handled tool-based tasks using the configured LLM.\")\n",
    "print(\"   - Continue testing edge prompts to validate robustness.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Google ADK Multi-Provider",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
