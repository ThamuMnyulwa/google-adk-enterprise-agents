{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96eb71b7",
   "metadata": {},
   "source": [
    "# Multi-Agent Workflows\n",
    "\n",
    "## Coordinated Agent Systems for Complex Tasks\n",
    "\n",
    "**Module Duration:** 15 minutes | **Focus:** Workflow orchestration and agent coordination\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "Master multi-agent coordination patterns for complex business processes:\n",
    "\n",
    "- **Sequential Workflows:** Step-by-step task progression with state management\n",
    "- **Parallel Processing:** Concurrent agent execution with coordination\n",
    "- **Agent Handoffs:** Clean state transfer between specialized agents\n",
    "- **Workflow Orchestration:** Complex business process automation\n",
    "- **Result Aggregation:** Combining outputs from multiple agents\n",
    "\n",
    "**What You'll Build:**\n",
    "- Sequential workflow system with state management\n",
    "- Parallel agent coordination framework\n",
    "- Agent handoff and state transfer mechanisms\n",
    "- Complex business process automation\n",
    "- Result validation and aggregation patterns\n",
    "\n",
    "This covers orchestration patterns used in enterprise AI automation systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1554de4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– MULTI-AGENT WORKFLOWS\n",
      "===================================\n",
      "Session: 2025-06-16 14:57:33\n",
      "Focus: Coordinated agent systems for complex tasks\n",
      "\n",
      "âœ… Multi-agent workflow foundation ready:\n",
      "   Workflow orchestration engine\n",
      "   Task status and result tracking\n",
      "   Shared context management\n"
     ]
    }
   ],
   "source": [
    "# Multi-Agent Workflow Foundation\n",
    "import asyncio\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Any, Optional, Union, Callable\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"ðŸ¤– MULTI-AGENT WORKFLOWS\")\n",
    "print(\"=\" * 35)\n",
    "print(f\"Session: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"Focus: Coordinated agent systems for complex tasks\")\n",
    "print()\n",
    "\n",
    "class TaskStatus(Enum):\n",
    "    \"\"\"Task execution status\"\"\"\n",
    "    PENDING = \"pending\"\n",
    "    IN_PROGRESS = \"in_progress\"\n",
    "    COMPLETED = \"completed\"\n",
    "    FAILED = \"failed\"\n",
    "    WAITING = \"waiting\"\n",
    "\n",
    "class WorkflowStatus(Enum):\n",
    "    \"\"\"Workflow execution status\"\"\"\n",
    "    CREATED = \"created\"\n",
    "    RUNNING = \"running\"\n",
    "    COMPLETED = \"completed\"\n",
    "    FAILED = \"failed\"\n",
    "    PAUSED = \"paused\"\n",
    "\n",
    "@dataclass\n",
    "class TaskResult:\n",
    "    \"\"\"Individual task execution result\"\"\"\n",
    "    task_id: str\n",
    "    agent_id: str\n",
    "    status: TaskStatus\n",
    "    result: Any\n",
    "    error: Optional[str] = None\n",
    "    execution_time: float = 0.0\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "@dataclass\n",
    "class WorkflowContext:\n",
    "    \"\"\"Shared context across workflow execution\"\"\"\n",
    "    workflow_id: str\n",
    "    created_at: str\n",
    "    status: WorkflowStatus\n",
    "    current_step: int\n",
    "    total_steps: int\n",
    "    shared_data: Dict[str, Any] = field(default_factory=dict)\n",
    "    task_results: Dict[str, TaskResult] = field(default_factory=dict)\n",
    "    execution_history: List[Dict[str, Any]] = field(default_factory=list)\n",
    "\n",
    "class WorkflowOrchestrator:\n",
    "    \"\"\"Enterprise workflow orchestration engine\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.workflows = {}\n",
    "        self.active_tasks = {}\n",
    "        self.agent_registry = {}\n",
    "        \n",
    "    def register_agent(self, agent_id: str, agent_instance: Any):\n",
    "        \"\"\"Register agent for workflow participation\"\"\"\n",
    "        self.agent_registry[agent_id] = agent_instance\n",
    "        logger.info(f\"Registered agent: {agent_id}\")\n",
    "    \n",
    "    def create_workflow(self, workflow_definition: Dict[str, Any]) -> str:\n",
    "        \"\"\"Create new workflow instance\"\"\"\n",
    "        workflow_id = str(uuid.uuid4())\n",
    "        \n",
    "        context = WorkflowContext(\n",
    "            workflow_id=workflow_id,\n",
    "            created_at=datetime.now().isoformat(),\n",
    "            status=WorkflowStatus.CREATED,\n",
    "            current_step=0,\n",
    "            total_steps=len(workflow_definition.get('steps', []))\n",
    "        )\n",
    "        \n",
    "        self.workflows[workflow_id] = {\n",
    "            'context': context,\n",
    "            'definition': workflow_definition\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Created workflow: {workflow_id}\")\n",
    "        return workflow_id\n",
    "    \n",
    "    def get_workflow_status(self, workflow_id: str) -> Dict[str, Any]:\n",
    "        \"\"\"Get current workflow status and progress\"\"\"\n",
    "        if workflow_id not in self.workflows:\n",
    "            return {\"error\": \"Workflow not found\"}\n",
    "        \n",
    "        workflow = self.workflows[workflow_id]\n",
    "        context = workflow['context']\n",
    "        \n",
    "        return {\n",
    "            \"workflow_id\": workflow_id,\n",
    "            \"status\": context.status.value,\n",
    "            \"progress\": f\"{context.current_step}/{context.total_steps}\",\n",
    "            \"completion_percentage\": (context.current_step / max(context.total_steps, 1)) * 100,\n",
    "            \"task_results\": len(context.task_results),\n",
    "            \"shared_data_keys\": list(context.shared_data.keys())\n",
    "        }\n",
    "\n",
    "# Initialize orchestrator\n",
    "orchestrator = WorkflowOrchestrator()\n",
    "\n",
    "print(\"âœ… Multi-agent workflow foundation ready:\")\n",
    "print(\"   Workflow orchestration engine\")\n",
    "print(\"   Task status and result tracking\")\n",
    "print(\"   Shared context management\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f2d179",
   "metadata": {},
   "source": [
    "### Sequential Workflow Patterns\n",
    "\n",
    "Sequential workflows process tasks in order, with each step depending on previous results:\n",
    "\n",
    "**Key Features:**\n",
    "- **State Management:** Passing data between sequential steps\n",
    "- **Dependency Resolution:** Ensuring prerequisites are met\n",
    "- **Error Propagation:** Handling failures in the chain\n",
    "- **Progress Tracking:** Monitoring workflow advancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12d6c0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Created workflow: 956cdd50-8f8b-4da5-80bd-0bb37c6ac787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”„ Sequential workflow created:\n",
      "   Workflow ID: 956cdd50\n",
      "   Steps: 5\n",
      "   Agents: data_processor, analyzer, reporter\n"
     ]
    }
   ],
   "source": [
    "# Sequential Workflow Implementation\n",
    "import time\n",
    "\n",
    "class SequentialWorkflow:\n",
    "    \"\"\"Sequential task execution with state management\"\"\"\n",
    "    \n",
    "    def __init__(self, orchestrator: WorkflowOrchestrator):\n",
    "        self.orchestrator = orchestrator\n",
    "    \n",
    "    async def execute_step(self, workflow_id: str, step_definition: Dict[str, Any], context: WorkflowContext) -> TaskResult:\n",
    "        \"\"\"Execute individual workflow step\"\"\"\n",
    "        task_id = f\"{workflow_id}_step_{context.current_step}\"\n",
    "        agent_id = step_definition['agent_id']\n",
    "        task_type = step_definition['task_type']\n",
    "        parameters = step_definition.get('parameters', {})\n",
    "        \n",
    "        # Add shared context to parameters\n",
    "        parameters['shared_data'] = context.shared_data\n",
    "        parameters['previous_results'] = list(context.task_results.values())\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Simulate agent task execution\n",
    "            if agent_id == \"data_processor\":\n",
    "                result = await self._process_data_task(task_type, parameters)\n",
    "            elif agent_id == \"analyzer\":\n",
    "                result = await self._analyze_task(task_type, parameters)\n",
    "            elif agent_id == \"reporter\":\n",
    "                result = await self._report_task(task_type, parameters)\n",
    "            else:\n",
    "                result = await self._generic_task(agent_id, task_type, parameters)\n",
    "            \n",
    "            execution_time = time.time() - start_time\n",
    "            \n",
    "            task_result = TaskResult(\n",
    "                task_id=task_id,\n",
    "                agent_id=agent_id,\n",
    "                status=TaskStatus.COMPLETED,\n",
    "                result=result,\n",
    "                execution_time=execution_time,\n",
    "                metadata={\"task_type\": task_type}\n",
    "            )\n",
    "            \n",
    "            # Update shared context with results\n",
    "            if 'output_key' in step_definition:\n",
    "                context.shared_data[step_definition['output_key']] = result\n",
    "            \n",
    "            logger.info(f\"Completed step {context.current_step}: {task_type} by {agent_id}\")\n",
    "            return task_result\n",
    "            \n",
    "        except Exception as e:\n",
    "            execution_time = time.time() - start_time\n",
    "            error_result = TaskResult(\n",
    "                task_id=task_id,\n",
    "                agent_id=agent_id,\n",
    "                status=TaskStatus.FAILED,\n",
    "                result=None,\n",
    "                error=str(e),\n",
    "                execution_time=execution_time,\n",
    "                metadata={\"task_type\": task_type}\n",
    "            )\n",
    "            \n",
    "            logger.error(f\"Failed step {context.current_step}: {str(e)}\")\n",
    "            return error_result\n",
    "    \n",
    "    async def _process_data_task(self, task_type: str, parameters: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Simulate data processing agent task\"\"\"\n",
    "        await asyncio.sleep(0.1)  # Simulate processing time\n",
    "        \n",
    "        if task_type == \"extract_data\":\n",
    "            return {\n",
    "                \"extracted_records\": 1250,\n",
    "                \"data_sources\": [\"database\", \"api\", \"files\"],\n",
    "                \"extraction_time\": \"0.1s\",\n",
    "                \"quality_score\": 0.95\n",
    "            }\n",
    "        elif task_type == \"clean_data\":\n",
    "            previous_count = parameters.get('shared_data', {}).get('extracted_records', 1000)\n",
    "            cleaned_count = int(previous_count * 0.92)  # 8% data cleaning loss\n",
    "            return {\n",
    "                \"cleaned_records\": cleaned_count,\n",
    "                \"removed_duplicates\": previous_count - cleaned_count,\n",
    "                \"validation_passed\": True,\n",
    "                \"data_quality\": \"high\"\n",
    "            }\n",
    "        else:\n",
    "            return {\"status\": \"completed\", \"task_type\": task_type}\n",
    "    \n",
    "    async def _analyze_task(self, task_type: str, parameters: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Simulate analysis agent task\"\"\"\n",
    "        await asyncio.sleep(0.15)  # Simulate analysis time\n",
    "        \n",
    "        if task_type == \"statistical_analysis\":\n",
    "            record_count = parameters.get('shared_data', {}).get('cleaned_records', 1000)\n",
    "            return {\n",
    "                \"total_records\": record_count,\n",
    "                \"mean_value\": 145.7,\n",
    "                \"std_deviation\": 23.4,\n",
    "                \"outliers_detected\": 12,\n",
    "                \"correlation_score\": 0.73,\n",
    "                \"insights\": [\"Strong positive correlation\", \"Seasonal trends detected\"]\n",
    "            }\n",
    "        elif task_type == \"trend_analysis\":\n",
    "            return {\n",
    "                \"trend_direction\": \"upward\",\n",
    "                \"growth_rate\": \"12.5%\",\n",
    "                \"confidence_level\": 0.87,\n",
    "                \"forecast_period\": \"Q1 2025\",\n",
    "                \"key_drivers\": [\"market expansion\", \"product innovation\"]\n",
    "            }\n",
    "        else:\n",
    "            return {\"status\": \"analyzed\", \"task_type\": task_type}\n",
    "    \n",
    "    async def _report_task(self, task_type: str, parameters: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Simulate reporting agent task\"\"\"\n",
    "        await asyncio.sleep(0.08)  # Simulate report generation\n",
    "        \n",
    "        if task_type == \"generate_report\":\n",
    "            shared_data = parameters.get('shared_data', {})\n",
    "            return {\n",
    "                \"report_id\": f\"RPT_{uuid.uuid4().hex[:8]}\",\n",
    "                \"pages_generated\": 15,\n",
    "                \"charts_created\": 8,\n",
    "                \"executive_summary\": \"Data analysis reveals strong growth trends with high confidence levels\",\n",
    "                \"data_sources\": shared_data.get('data_sources', []),\n",
    "                \"report_quality\": \"high\",\n",
    "                \"generated_at\": datetime.now().isoformat()\n",
    "            }\n",
    "        else:\n",
    "            return {\"status\": \"reported\", \"task_type\": task_type}\n",
    "    \n",
    "    async def _generic_task(self, agent_id: str, task_type: str, parameters: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Generic task execution for unknown agents\"\"\"\n",
    "        await asyncio.sleep(0.05)\n",
    "        return {\n",
    "            \"agent_id\": agent_id,\n",
    "            \"task_type\": task_type,\n",
    "            \"status\": \"completed\",\n",
    "            \"parameters_received\": len(parameters),\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "    \n",
    "    async def execute_workflow(self, workflow_id: str) -> WorkflowContext:\n",
    "        \"\"\"Execute complete sequential workflow\"\"\"\n",
    "        if workflow_id not in self.orchestrator.workflows:\n",
    "            raise ValueError(f\"Workflow {workflow_id} not found\")\n",
    "        \n",
    "        workflow = self.orchestrator.workflows[workflow_id]\n",
    "        context = workflow['context']\n",
    "        steps = workflow['definition']['steps']\n",
    "        \n",
    "        context.status = WorkflowStatus.RUNNING\n",
    "        \n",
    "        try:\n",
    "            for i, step in enumerate(steps):\n",
    "                context.current_step = i + 1\n",
    "                \n",
    "                # Execute step\n",
    "                result = await self.execute_step(workflow_id, step, context)\n",
    "                context.task_results[result.task_id] = result\n",
    "                \n",
    "                # Record execution history\n",
    "                context.execution_history.append({\n",
    "                    \"step\": i + 1,\n",
    "                    \"agent_id\": result.agent_id,\n",
    "                    \"status\": result.status.value,\n",
    "                    \"execution_time\": result.execution_time,\n",
    "                    \"timestamp\": datetime.now().isoformat()\n",
    "                })\n",
    "                \n",
    "                # Check for failure\n",
    "                if result.status == TaskStatus.FAILED:\n",
    "                    context.status = WorkflowStatus.FAILED\n",
    "                    logger.error(f\"Workflow {workflow_id} failed at step {i + 1}\")\n",
    "                    return context\n",
    "            \n",
    "            context.status = WorkflowStatus.COMPLETED\n",
    "            logger.info(f\"Workflow {workflow_id} completed successfully\")\n",
    "            return context\n",
    "            \n",
    "        except Exception as e:\n",
    "            context.status = WorkflowStatus.FAILED\n",
    "            logger.error(f\"Workflow {workflow_id} execution failed: {str(e)}\")\n",
    "            return context\n",
    "\n",
    "# Initialize sequential workflow engine\n",
    "sequential_engine = SequentialWorkflow(orchestrator)\n",
    "\n",
    "# Create sample sequential workflow\n",
    "workflow_definition = {\n",
    "    \"name\": \"Data Analysis Pipeline\",\n",
    "    \"description\": \"Extract, clean, analyze, and report on business data\",\n",
    "    \"steps\": [\n",
    "        {\n",
    "            \"agent_id\": \"data_processor\",\n",
    "            \"task_type\": \"extract_data\",\n",
    "            \"output_key\": \"extracted_records\",\n",
    "            \"parameters\": {\"source\": \"business_db\"}\n",
    "        },\n",
    "        {\n",
    "            \"agent_id\": \"data_processor\", \n",
    "            \"task_type\": \"clean_data\",\n",
    "            \"output_key\": \"cleaned_records\",\n",
    "            \"parameters\": {\"validation_rules\": [\"remove_duplicates\", \"check_nulls\"]}\n",
    "        },\n",
    "        {\n",
    "            \"agent_id\": \"analyzer\",\n",
    "            \"task_type\": \"statistical_analysis\",\n",
    "            \"output_key\": \"analysis_results\",\n",
    "            \"parameters\": {\"analysis_type\": \"comprehensive\"}\n",
    "        },\n",
    "        {\n",
    "            \"agent_id\": \"analyzer\",\n",
    "            \"task_type\": \"trend_analysis\", \n",
    "            \"output_key\": \"trend_results\",\n",
    "            \"parameters\": {\"period\": \"quarterly\"}\n",
    "        },\n",
    "        {\n",
    "            \"agent_id\": \"reporter\",\n",
    "            \"task_type\": \"generate_report\",\n",
    "            \"output_key\": \"final_report\",\n",
    "            \"parameters\": {\"format\": \"executive_summary\"}\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create workflow instance\n",
    "workflow_id = orchestrator.create_workflow(workflow_definition)\n",
    "\n",
    "print(\"\\nðŸ”„ Sequential workflow created:\")\n",
    "print(f\"   Workflow ID: {workflow_id[:8]}\")\n",
    "print(f\"   Steps: {len(workflow_definition['steps'])}\")\n",
    "print(\"   Agents: data_processor, analyzer, reporter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9500ffa",
   "metadata": {},
   "source": [
    "### Parallel Agent Coordination\n",
    "\n",
    "Parallel workflows execute multiple agents simultaneously with coordination:\n",
    "\n",
    "**Coordination Patterns:**\n",
    "- **Concurrent Execution:** Multiple agents working simultaneously\n",
    "- **Synchronization Points:** Waiting for all agents to complete\n",
    "- **Resource Sharing:** Managing shared data access\n",
    "- **Result Aggregation:** Combining parallel execution results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f8ea070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš¡ Parallel workflow engine ready:\n",
      "   Concurrent task execution\n",
      "   Result aggregation and coordination\n",
      "   Thread-safe context management\n"
     ]
    }
   ],
   "source": [
    "# Parallel Agent Coordination System\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "class ParallelWorkflow:\n",
    "    \"\"\"Parallel agent execution with coordination\"\"\"\n",
    "    \n",
    "    def __init__(self, orchestrator: WorkflowOrchestrator):\n",
    "        self.orchestrator = orchestrator\n",
    "        self.max_concurrent = 5\n",
    "    \n",
    "    async def execute_parallel_group(self, workflow_id: str, group_definition: List[Dict[str, Any]], \n",
    "                                   context: WorkflowContext) -> List[TaskResult]:\n",
    "        \"\"\"Execute group of tasks in parallel\"\"\"\n",
    "        tasks = []\n",
    "        \n",
    "        for i, task_def in enumerate(group_definition):\n",
    "            task_id = f\"{workflow_id}_parallel_{context.current_step}_{i}\"\n",
    "            task = self._create_parallel_task(task_id, task_def, context)\n",
    "            tasks.append(task)\n",
    "        \n",
    "        # Execute all tasks concurrently\n",
    "        start_time = time.time()\n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        # Process results\n",
    "        task_results = []\n",
    "        for i, result in enumerate(results):\n",
    "            if isinstance(result, Exception):\n",
    "                task_result = TaskResult(\n",
    "                    task_id=f\"{workflow_id}_parallel_{context.current_step}_{i}\",\n",
    "                    agent_id=group_definition[i]['agent_id'],\n",
    "                    status=TaskStatus.FAILED,\n",
    "                    result=None,\n",
    "                    error=str(result),\n",
    "                    execution_time=total_time\n",
    "                )\n",
    "            else:\n",
    "                task_result = result\n",
    "            \n",
    "            task_results.append(task_result)\n",
    "        \n",
    "        logger.info(f\"Completed parallel group: {len(task_results)} tasks in {total_time:.2f}s\")\n",
    "        return task_results\n",
    "    \n",
    "    async def _create_parallel_task(self, task_id: str, task_def: Dict[str, Any], \n",
    "                                  context: WorkflowContext) -> TaskResult:\n",
    "        \"\"\"Create individual parallel task\"\"\"\n",
    "        agent_id = task_def['agent_id']\n",
    "        task_type = task_def['task_type']\n",
    "        parameters = task_def.get('parameters', {})\n",
    "        \n",
    "        # Add context (thread-safe read-only access)\n",
    "        parameters['shared_data'] = dict(context.shared_data)  # Copy for thread safety\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Route to appropriate agent simulation\n",
    "            if agent_id == \"validator\":\n",
    "                result = await self._validation_task(task_type, parameters)\n",
    "            elif agent_id == \"enricher\":\n",
    "                result = await self._enrichment_task(task_type, parameters)\n",
    "            elif agent_id == \"classifier\":\n",
    "                result = await self._classification_task(task_type, parameters)\n",
    "            elif agent_id == \"scorer\":\n",
    "                result = await self._scoring_task(task_type, parameters)\n",
    "            else:\n",
    "                result = await self._generic_parallel_task(agent_id, task_type, parameters)\n",
    "            \n",
    "            execution_time = time.time() - start_time\n",
    "            \n",
    "            return TaskResult(\n",
    "                task_id=task_id,\n",
    "                agent_id=agent_id,\n",
    "                status=TaskStatus.COMPLETED,\n",
    "                result=result,\n",
    "                execution_time=execution_time,\n",
    "                metadata={\"task_type\": task_type, \"parallel\": True}\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            execution_time = time.time() - start_time\n",
    "            return TaskResult(\n",
    "                task_id=task_id,\n",
    "                agent_id=agent_id,\n",
    "                status=TaskStatus.FAILED,\n",
    "                result=None,\n",
    "                error=str(e),\n",
    "                execution_time=execution_time,\n",
    "                metadata={\"task_type\": task_type, \"parallel\": True}\n",
    "            )\n",
    "    \n",
    "    async def _validation_task(self, task_type: str, parameters: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Simulate data validation agent\"\"\"\n",
    "        await asyncio.sleep(0.12)  # Simulate validation time\n",
    "        \n",
    "        record_count = parameters.get('shared_data', {}).get('cleaned_records', 1000)\n",
    "        validation_rate = 0.94  # 94% pass validation\n",
    "        \n",
    "        return {\n",
    "            \"validation_type\": task_type,\n",
    "            \"records_validated\": record_count,\n",
    "            \"passed_validation\": int(record_count * validation_rate),\n",
    "            \"failed_validation\": int(record_count * (1 - validation_rate)),\n",
    "            \"validation_rules_applied\": 8,\n",
    "            \"confidence_score\": 0.94,\n",
    "            \"issues_found\": [\"missing_dates\", \"invalid_formats\"]\n",
    "        }\n",
    "    \n",
    "    async def _enrichment_task(self, task_type: str, parameters: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Simulate data enrichment agent\"\"\"\n",
    "        await asyncio.sleep(0.18)  # Simulate enrichment time\n",
    "        \n",
    "        record_count = parameters.get('shared_data', {}).get('cleaned_records', 1000)\n",
    "        enrichment_rate = 0.87  # 87% successfully enriched\n",
    "        \n",
    "        return {\n",
    "            \"enrichment_type\": task_type,\n",
    "            \"records_processed\": record_count,\n",
    "            \"successfully_enriched\": int(record_count * enrichment_rate),\n",
    "            \"enrichment_sources\": [\"external_api\", \"reference_db\", \"ml_model\"],\n",
    "            \"new_fields_added\": 5,\n",
    "            \"data_completeness\": 0.92,\n",
    "            \"enrichment_quality\": \"high\"\n",
    "        }\n",
    "    \n",
    "    async def _classification_task(self, task_type: str, parameters: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Simulate classification agent\"\"\"\n",
    "        await asyncio.sleep(0.14)  # Simulate classification time\n",
    "        \n",
    "        record_count = parameters.get('shared_data', {}).get('cleaned_records', 1000)\n",
    "        \n",
    "        return {\n",
    "            \"classification_type\": task_type,\n",
    "            \"records_classified\": record_count,\n",
    "            \"categories\": {\n",
    "                \"high_value\": int(record_count * 0.15),\n",
    "                \"medium_value\": int(record_count * 0.35), \n",
    "                \"low_value\": int(record_count * 0.50)\n",
    "            },\n",
    "            \"classification_confidence\": 0.89,\n",
    "            \"model_version\": \"v2.1.3\",\n",
    "            \"accuracy_score\": 0.91\n",
    "        }\n",
    "    \n",
    "    async def _scoring_task(self, task_type: str, parameters: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Simulate scoring agent\"\"\"\n",
    "        await asyncio.sleep(0.10)  # Simulate scoring time\n",
    "        \n",
    "        record_count = parameters.get('shared_data', {}).get('cleaned_records', 1000)\n",
    "        \n",
    "        return {\n",
    "            \"scoring_type\": task_type,\n",
    "            \"records_scored\": record_count,\n",
    "            \"average_score\": 73.2,\n",
    "            \"score_distribution\": {\n",
    "                \"0-25\": int(record_count * 0.08),\n",
    "                \"26-50\": int(record_count * 0.22),\n",
    "                \"51-75\": int(record_count * 0.45),\n",
    "                \"76-100\": int(record_count * 0.25)\n",
    "            },\n",
    "            \"scoring_model\": \"ensemble_v1.2\",\n",
    "            \"feature_importance\": [\"recency\", \"frequency\", \"monetary\"]\n",
    "        }\n",
    "    \n",
    "    async def _generic_parallel_task(self, agent_id: str, task_type: str, \n",
    "                                   parameters: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Generic parallel task execution\"\"\"\n",
    "        await asyncio.sleep(0.05)\n",
    "        return {\n",
    "            \"agent_id\": agent_id,\n",
    "            \"task_type\": task_type,\n",
    "            \"status\": \"completed\",\n",
    "            \"parallel_execution\": True,\n",
    "            \"parameters_processed\": len(parameters),\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "    \n",
    "    def aggregate_parallel_results(self, results: List[TaskResult]) -> Dict[str, Any]:\n",
    "        \"\"\"Aggregate results from parallel execution\"\"\"\n",
    "        successful_tasks = [r for r in results if r.status == TaskStatus.COMPLETED]\n",
    "        failed_tasks = [r for r in results if r.status == TaskStatus.FAILED]\n",
    "        \n",
    "        total_execution_time = max([r.execution_time for r in results]) if results else 0\n",
    "        \n",
    "        # Aggregate metrics from successful tasks\n",
    "        aggregated_data = {\n",
    "            \"total_tasks\": len(results),\n",
    "            \"successful_tasks\": len(successful_tasks),\n",
    "            \"failed_tasks\": len(failed_tasks),\n",
    "            \"success_rate\": len(successful_tasks) / len(results) if results else 0,\n",
    "            \"total_execution_time\": total_execution_time,\n",
    "            \"agent_performance\": {}\n",
    "        }\n",
    "        \n",
    "        # Aggregate by agent type\n",
    "        for result in successful_tasks:\n",
    "            agent_id = result.agent_id\n",
    "            if agent_id not in aggregated_data[\"agent_performance\"]:\n",
    "                aggregated_data[\"agent_performance\"][agent_id] = {\n",
    "                    \"tasks_completed\": 0,\n",
    "                    \"average_time\": 0,\n",
    "                    \"results\": []\n",
    "                }\n",
    "            \n",
    "            agent_perf = aggregated_data[\"agent_performance\"][agent_id]\n",
    "            agent_perf[\"tasks_completed\"] += 1\n",
    "            agent_perf[\"average_time\"] = (agent_perf[\"average_time\"] + result.execution_time) / agent_perf[\"tasks_completed\"]\n",
    "            agent_perf[\"results\"].append(result.result)\n",
    "        \n",
    "        return aggregated_data\n",
    "\n",
    "# Initialize parallel workflow engine\n",
    "parallel_engine = ParallelWorkflow(orchestrator)\n",
    "\n",
    "# Test parallel execution\n",
    "print(\"\\nâš¡ Parallel workflow engine ready:\")\n",
    "print(\"   Concurrent task execution\")\n",
    "print(\"   Result aggregation and coordination\")\n",
    "print(\"   Thread-safe context management\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef08c56",
   "metadata": {},
   "source": [
    "### Agent Handoffs and State Transfer\n",
    "\n",
    "Complex workflows require smooth transitions between specialized agents:\n",
    "\n",
    "**Handoff Patterns:**\n",
    "- **State Serialization:** Capturing complete agent state\n",
    "- **Context Transfer:** Moving relevant data between agents\n",
    "- **Validation Checkpoints:** Ensuring data integrity during handoffs\n",
    "- **Rollback Mechanisms:** Handling handoff failures gracefully"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Google ADK Multi-Provider",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
